{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ü§ñ Fine-Tuning T5 for Product Review Generation\n",
        "\n",
        "In this interactive lab, we'll explore the exciting task of generating product reviews using the T5 (Text-to-Text Transfer Transformer) model. We'll dive into data preparation, model training, and ultimately, review generation."
      ],
      "metadata": {
        "id": "ART3jcgfQnll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup and Installation\n",
        "\n",
        "First things first, we need to install the required libraries to ensure our environment is ready for the tasks ahead."
      ],
      "metadata": {
        "id": "Dt_DnxuxQrYQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RnkcEruL16_"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.25.1\n",
        "!pip install transformers\n",
        "!pip install datasets===2.13.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing Libraries üìö\n",
        "\n",
        "Let's import all the necessary modules that will help us load datasets, process data, and utilize the T5 model.\n"
      ],
      "metadata": {
        "id": "Pm8dgfLhQtU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "wry0xt5IMT9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preparation üìã\n",
        "\n",
        "Our journey begins with preparing our dataset. We'll use a subset of Amazon product reviews for our analysis and training.\n",
        "\n",
        "Loading and Merging Datasets\n",
        "We replace the unavailable \"amazon_us_reviews\" with a similar dataset and merge metadata with review data."
      ],
      "metadata": {
        "id": "cP54RPUuQxDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Amazon removed the \"amazon_us_reviews\" dataset, so we'll have to use a replacement here.\n",
        "dataset_category = \"Software\" # \"Electronics\" you can also choose electronics like in the lesson, but the dataset is bigger and loading will take longer\n",
        "\n",
        "meta_ds = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_meta_{dataset_category}\", split='full').to_pandas()[['parent_asin', 'title']]\n",
        "review_ds = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_review_{dataset_category}\", split='full').to_pandas()[['parent_asin', 'rating', 'text', 'verified_purchase']]\n",
        "\n",
        "ds = meta_ds.merge(review_ds, on='parent_asin', how='inner').drop(columns=\"parent_asin\")\n",
        "ds = ds.rename(columns={\"rating\":\"star_rating\", \"title\":\"product_title\", \"text\":\"review_body\"})\n",
        "\n",
        "ds = ds[ds['verified_purchase'] & (ds['review_body'].map(len) > 100)].sample(100_000)\n",
        "ds"
      ],
      "metadata": {
        "id": "-MPROC5RZVOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding and Splitting\n",
        "Next, we encode our star_rating column and split our dataset into training and testing sets."
      ],
      "metadata": {
        "id": "BMGGnVcQRF0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "dataset = Dataset.from_pandas(ds)\n",
        "\n",
        "# encoding the 'star_rating' column\n",
        "dataset = dataset.class_encode_column(\"star_rating\")\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"star_rating\")\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "yxk2cc2eMVhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Preparation üõ†Ô∏è\n",
        "\n",
        "Now, let's prepare our T5 model for training.\n",
        "\n",
        "###Tokenizer Initialization"
      ],
      "metadata": {
        "id": "8czCZhj6Rmn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')|"
      ],
      "metadata": {
        "id": "BRdTCTeQMWax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preprocessing Function\n",
        "We define a function to preprocess our data, preparing it for the model."
      ],
      "metadata": {
        "id": "8uBv8DDUR748"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to preprocess the data\n",
        "def preprocess_data(examples):\n",
        "    examples['prompt'] = [f\"review: {example['product_title']}, {example['star_rating']} Stars!\" for example in examples]\n",
        "    examples['response'] = [f\"{example['review_headline']} {example['review_body']}\" for example in examples]\n",
        "\n",
        "    inputs = tokenizer(examples['prompt'], padding='max_length', truncation=True, max_length=128)\n",
        "    targets = tokenizer(examples['response'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "    # Set -100 at the padding positions of target tokens\n",
        "    target_input_ids = []\n",
        "    for ids in targets['input_ids']:\n",
        "        target_input_ids.append([id if id != tokenizer.pad_token_id else -100 for id in ids])\n",
        "\n",
        "    inputs.update({'labels': target_input_ids})\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "XAoQgenJMXmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing Datasets"
      ],
      "metadata": {
        "id": "pg45zv6YR-xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "0Y3XRQRgMY7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fine-Tuning the Model üéØ\n",
        "\n",
        "With our data ready, we proceed to fine-tune the T5 model on our dataset."
      ],
      "metadata": {
        "id": "rkOG7tDASGMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "TRAINING_OUTPUT = \"./models/t5_fine_tuned_reviews\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=TRAINING_OUTPUT,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    save_strategy='epoch',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XP1BI2A_MaXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving and Loading the Model üíæ\n",
        "\n",
        "After training, we save our model for later use and demonstrate how to load it."
      ],
      "metadata": {
        "id": "h7KSJRzVSKo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(TRAINING_OUTPUT)"
      ],
      "metadata": {
        "id": "1oOegLTcMblg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the fine-tuned model\n",
        "model = T5ForConditionalGeneration.from_pretrained(TRAINING_OUTPUT)\n",
        "\n",
        "# or get it directly trained from here:\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"TheFuzzyScientist/T5-base_Amazon-product-reviews\")"
      ],
      "metadata": {
        "id": "Wi6e0UPYMdT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generating Reviews ‚úçÔ∏è\n",
        "\n",
        "Finally, we use our fine-tuned model to generate reviews for new products."
      ],
      "metadata": {
        "id": "6YOyY4gXSO7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to generate reviews\n",
        "def generate_review(text):\n",
        "    inputs = tokenizer(\"review: \" + text, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=128, no_repeat_ngram_size=3, num_beams=6, early_stopping=True)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "iMKmA5aXMehK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating reviews for random products\n",
        "random_products = test_dataset.shuffle(42).select(range(10))['product_title']\n",
        "\n",
        "print(generate_review(random_products[0] + \", 3 Stars!\"))\n",
        "print(generate_review(random_products[1] + \", 5 Stars!\"))\n",
        "print(generate_review(random_products[2] + \", 2 Stars!\"))"
      ],
      "metadata": {
        "id": "dqWMFXkCMf_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion üöÄ\n",
        "\n",
        "Congratulations! You've just completed a hands-on project on fine-tuning T5 for generating product reviews. Experiment further with different product categories or tweak the model parameters to see how it affects the output. Happy coding!"
      ],
      "metadata": {
        "id": "QOnPcIaLSS65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Next Steps: Elevate Your Skills with Advanced Techniques\n",
        "\n",
        "Congratulations on reaching this far! If you're keen to expand your expertise and dive deeper into the world of Large Language Models (LLMs), our next course is designed just for you.\n",
        "\n",
        "### üåü Use LLMs Smarter: Scale Gen AI, ML-Ops & Cost Efficiency\n",
        "In a world where AI and machine learning are revolutionizing industries, the ability to deploy and manage massive models like Llama, Mistral, and Gemma efficiently is invaluable.\n",
        "\n",
        "This course is tailored to equip you with the knowledge and skills to:\n",
        "\n",
        "* **Deploy Huge Models**: Learn the ins and outs of working with some of the largest models available, understanding their architecture and how they can be leveraged for your projects.\n",
        "* **Scale Across Clusters**: Discover strategies for scaling these behemoths across clusters of machines without sacrificing performance, ensuring seamless operation.\n",
        "* **Optimize Response Times**: Achieve response times in the milliseconds while maintaining the delicate balance between accuracy and speed.\n",
        "* **Balance Accuracy, Speed, & Cost**: Master the art of cost efficiency without compromising on performance, utilizing the latest and most powerful technologies.\n",
        "\n",
        "### üéì Why This Course?\n",
        "This course goes beyond the basics, diving into the practical aspects of deploying and optimizing LLMs at scale. Whether you're working on cutting-edge research or developing solutions for real-world problems, the insights and techniques covered here will be invaluable.\n",
        "\n",
        "### üí° Take the Leap\n",
        "If you're intrigued by the possibilities and ready to take your skills to the next level, take 2 minutes to explore this course further. As a token of our gratitude for completing the current course, we're offering an exclusive discount‚Äîbetter than what you might find elsewhere‚Äîfor this next step in your journey.\n",
        "\n",
        "# üîó Check out the course [here](https://www.udemy.com/course/deploy-ai-smarter-llm-scalability-ml-ops-cost-efficiency/?referralCode=ADC24A974EEC326467E6/?couponCode=D0627D21CFA8214F8939)\n",
        "\n",
        "Use this cuppon code if the discount is not applied automatically: **D0627D21CFA8214F8939**\n",
        "\n",
        "\n",
        "Embrace the opportunity to become a proficient practitioner in deploying, scaling, and optimizing Large Language Models. Your journey into the advanced realms of AI and machine learning starts now!"
      ],
      "metadata": {
        "id": "E2xECPNvSVDh"
      }
    }
  ]
}