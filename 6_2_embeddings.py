# -*- coding: utf-8 -*-
"""6.2 Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQLidcWx-dj8SH1bCOfzB6wUpl2bso4l

# 🧠 **Understanding Embeddings with BERT**

In this notebook, we go into the world of embeddings, utilizing the BERT model to understand the semantic relationships between words in different contexts.

## 🛠️ Setup and Installation

Start by installing the necessary libraries to ensure all functionalities are available.
"""

!pip install transformers==4.29.2
!pip install scipy==1.7.3

"""## 📚 Importing Libraries

Import essential modules for our tasks.
"""

from transformers import BertModel, AutoTokenizer
from scipy.spatial.distance import cosine

"""## 🤖 Model Setup

Load the pre-trained BERT model and tokenizer. This model will help us extract embeddings for our analysis.
"""

# Defining the model name
model_name = "bert-base-cased"

# Loading the pre-trained model and tokenizer
model = BertModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""
## 📝 Function Definition: Predict

Define a function that encodes input text into tensors, which are then fed to the model to obtain embeddings."""

# Defining a function to encode the input text and get model predictions
def predict(text):
    encoded_inputs = tokenizer(text, return_tensors="pt")
    return model(**encoded_inputs)[0]

"""## 📃 Defining the Sentences

Set up sentences to analyze. The
"""

# Defining the sentences
sentence1 = "There was a fly drinking from my soup"
sentence2 = "There is a fly swimming in my juice"
# sentence2 = "To become a commercial pilot, he had to fly for 1500 hours." # second fly example

# Tokenizing the sentences
tokens1 = tokenizer.tokenize(sentence1)
tokens2 = tokenizer.tokenize(sentence2)

"""## 🔍 Tokenization and Model Predictions

Tokenize the sentences and obtain predictions (embeddings) from the model.
"""

# Getting model predictions for the sentences
out1 = predict(sentence1)
out2 = predict(sentence2)

"""## 🔄 Extracting Embeddings

Extract embeddings specifically for the word "fly" from both sentences.
"""

# Extracting embeddings for the word 'fly' in both sentences
emb1 = out1[0:, tokens1.index("fly"), :].detach()[0]
emb2 = out2[0:, tokens2.index("fly"), :].detach()[0]

# emb1 = out1[0:, 3, :].detach()
# emb2 = out2[0:, 3, :].detach()

"""## 📊 Calculating Cosine Similarity

Calculate the cosine similarity between the embeddings of the word "fly" from both sentences to measure how context affects meaning.
"""

# Calculating the cosine similarity between the embeddings
cosine(emb1, emb2)

"""## 🌟 Conclusion

This notebook has guided you through the process of extracting and comparing word embeddings using BERT. Such techniques are fundamental in understanding word semantics and their usage across different contexts.

Experiment by changing the sentences or focusing on different words to see how the embeddings and their similarities vary!
"""